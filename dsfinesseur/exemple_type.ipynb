{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T15:59:05.985520Z",
     "start_time": "2020-03-24T15:59:05.961788Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"Fonctions assez génériques pour être utilisées dans tout le projet.\"\"\"\n",
    "import math\n",
    "import dateutil\n",
    "import statistics as stat\n",
    "from enum import Enum\n",
    "\n",
    "from numpy import datetime64\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEUIL_CATEGORIE_PAR_DEFAUT = 5\n",
    "\n",
    "\n",
    "__all__ = [ 'Nature', 'valeur_numpy_nulle', 'donner_type', 'sigmoide', 'inverse_sigmoide' ]\n",
    "\n",
    "\n",
    "class EnumManager(Enum):\n",
    "    \"\"\"Add-ons on Enum objects.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def has_name(cls, name):\n",
    "        \"\"\"Check if the name is in the enumeration. \"\"\"\n",
    "        return name in cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def has_value(cls, value):\n",
    "        \"\"\"Check if the value is in the enumeration. \"\"\"\n",
    "        return value in cls._value2member_map_\n",
    "\n",
    "    @classmethod\n",
    "    def generate_name_from_value(cls, element, has_default_value=True):\n",
    "        \"\"\"Ensure coherency in the enumeration \"\"\"\n",
    "        # default name\n",
    "        name = cls._member_names_.keys()[0] if has_default_value else None\n",
    "        if cls.has_name(element):\n",
    "            name = element\n",
    "        elif cls.has_value(element):\n",
    "            name = cls(element).name\n",
    "        else:\n",
    "            pass#print(\"Beware invalid role value. How is it possible?\")\n",
    "        return name\n",
    "\n",
    "\n",
    "class Nature(EnumManager):\n",
    "    \"\"\"Enumerations of allowed data types.\"\"\"\n",
    "    CATEGORY = \"Category\"\n",
    "    BINARY = \"Binary\"\n",
    "    ORDERED_CATEGORY = \"Ordered Category\"\n",
    "    UNORDERED_CATEGORY = \"Unordered Category\"\n",
    "    NUMBER = \"Number\"\n",
    "    DATETIME = \"Datetime\"\n",
    "\n",
    "\n",
    "def valeur_numpy_nulle(variable):\n",
    "    \"\"\" Pour savoir si une variable est NaN, il y a la fonction np.nan.\n",
    "    Toutefois, elle ne fonctionne que sur des valeurs numériques.\n",
    "    Cette fonction s'utilise sur tout type de fonction.\n",
    "        \n",
    "    Arguments d'entrées:\n",
    "        variable (Python Object)\n",
    "        \n",
    "    Arguments de sorties:\n",
    "        (bool)\n",
    "    \"\"\"\n",
    "    return variable in [None, \"\"] or str(variable).lower() == \"nan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T16:21:20.948173Z",
     "start_time": "2020-03-24T16:21:20.929489Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'est_date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-9dc88da9ec78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuzzy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'est_date' is not defined"
     ]
    }
   ],
   "source": [
    "# type cas datetime\n",
    "# https://stackoverflow.com/questions/25341945/check-if-string-has-date-any-format\n",
    "# https://docs.python.org/fr/3.7/library/datetime.html\n",
    "# https://stackoverflow.com/questions/13703720/converting-between-datetime-timestamp-and-datetime64\n",
    "from dateutil.parser import parse, parserinfo \n",
    "\n",
    "class CustomParserInfo(parserinfo):\n",
    "    \"\"\" TODO: real custom of date please\"\"\"\n",
    "    \n",
    "    MONTHS= [('Jan', 'January'), ('Feb', 'February'), ('Mar', 'March'), ('Apr', 'April'), ('May', 'May'), ('Jun', 'June'), ('Jul', 'July'), ('Aug', 'August'), ('Sep', 'Sept', 'September'), ('Oct', 'October'), ('Nov', 'November'), ('Dec', 'December')]\n",
    "\n",
    "    WEEKDAYS= [('Mon', 'Lundi', 'Monday'), ('Tue', 'Tuesday'), ('Wed', 'Wednesday'), ('Thu', 'Thursday'), ('Fri', 'Friday'), ('Sat', 'Saturday'), ('Sun', 'Sunday')]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_language(cls, language=None, kwargs=None):\n",
    "        return cls\n",
    "\n",
    "DATETIME_DTYPE = [np.dtype('datetime64'), np.dtype('datetime64[s]'), np.dtype('datetime64[ns]') ]\n",
    "\n",
    "    \n",
    "def est_numpy_date(element):#, string, fuzzy=False, language='eng', kwargs=None):\n",
    "    return any( y == date_type for date_type in DATETIME_DTYPE )\n",
    "\n",
    "    \n",
    "def est_date(element, str_kwargs=None):#, string, fuzzy=False, language='eng', kwargs=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    type_datetime = \n",
    "    type_series = isinstance(element, pd.Series)\n",
    "    type_str = isinstance(element, str)\n",
    "    type_list = isinstance(element, list)\n",
    "    \n",
    "    if type_datetime:\n",
    "        return True\n",
    "    \n",
    "    if type_series:\n",
    "        type_object = element.dtype == np.object\n",
    "        if type_object:\n",
    "            return any(map(est_numpy_date, element))\n",
    "        return est_numpy_date(element)\n",
    "    \n",
    "    if type_str:\n",
    "        return est_string_date(**{**(str_kwargs or {}), **{'string': element}})\n",
    "    \n",
    "    if type_list:\n",
    "        return any(map(est_date, element))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def est_date(string, fuzzy=False, language='eng', kwargs=None):\n",
    "\n",
    "y = pd.to_datetime(series).dtype\n",
    "z = ndate(y)\n",
    "a = y == np.datetime64\n",
    "b = est_date(series[0], fuzzy=False, language=None, kwargs=None)\n",
    "c = isinstance(series[0], (dt.timedelta, dt.date))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def est_date(string, fuzzy=False, language='eng', kwargs=None):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    parserinfo_instance = CustomParserInfo.from_language(language, kwargs)()\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy, parserinfo=parserinfo_instance)\n",
    "        return True\n",
    "\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T15:59:05.985520Z",
     "start_time": "2020-03-24T15:59:05.961788Z"
    }
   },
   "outputs": [],
   "source": [
    "def trouver_type(variable, seuil_categorie):\n",
    "    \"\"\"Déterminer le type d'une variable par calcul.\n",
    "\n",
    "    Arguments d'entrées:\n",
    "        variable (NumPy.array)\n",
    "        seuil_categorie (int): valeur qui détermine le type d'une variable\n",
    "            (numérique ou catégorique) suivant le nombre de valeurs distinctes.\n",
    "\n",
    "    Arguments de sorties:\n",
    "        (Enum)\n",
    "    \"\"\"\n",
    "    nombre_de_valeurs_distinctes = len(set(variable))\n",
    "    fuzzy=False\n",
    "    language='eng'\n",
    "    kwargs=None\n",
    "\n",
    "    type_variable_categorie = variable.dtype == 'object'\n",
    "    type_variable_datetime = variable.dtype == datetime64 or any(map(lambda s: is_date(s, fuzzy=fuzzy, language=language, kwargs=kwargs), variable))\n",
    "\n",
    "    if type_variable_datetime:\n",
    "        return Nature.DATETIME\n",
    "    if nombre_de_valeurs_distinctes == 2:\n",
    "        return Nature.BINARY\n",
    "    elif nombre_de_valeurs_distinctes < seuil_categorie or type_variable_categorie:\n",
    "        return Nature.CATEGORY\n",
    "\n",
    "    else:\n",
    "        return Nature.NUMBER\n",
    "\n",
    "\n",
    "def donner_type(variable, seuil_categorie=None, type_attitre=None):\n",
    "    \"\"\"Retourner le type d'une variable.\n",
    "\n",
    "    Arguments d'entrées:\n",
    "        variable (NumPy.array)\n",
    "        seuil_categorie (int): valeur qui détermine le type d'une variable\n",
    "            (numérique ou catégorique) suivant le nombre de valeurs distinctes.\n",
    "        type_attitre (str): possible type donnée par l'utilisateur à vérifier.\n",
    "\n",
    "    Arguments de sorties:\n",
    "        (Enum)\n",
    "    \"\"\"\n",
    "    if seuil_categorie is None:\n",
    "        seuil_categorie = SEUIL_CATEGORIE_PAR_DEFAUT\n",
    "    \n",
    "    type_attitre = Nature.generate_name_from_value(type_attitre, has_default_value=False)\n",
    "    if type_attitre is None:\n",
    "        return trouver_type(variable, seuil_categorie)\n",
    "    return type_attitre\n",
    "\n",
    "\n",
    "def sigmoide(x, valeur_sup=1.0, valeur_inf=.0, pente=1):\n",
    "    \"\"\"Personnalisable sigmoide.\n",
    "\n",
    "    Arguments d'entrées:\n",
    "        x (float)\n",
    "        valeur_sup, valeur_inf, pente (float): paramètres\n",
    "    \n",
    "    Arguments de sorties:\n",
    "        (float)\n",
    "    \"\"\"\n",
    "    return valeur_inf + ( valeur_sup - valeur_inf ) / ( 1 + math.exp(- pente * x) )\n",
    "\n",
    "\n",
    "def inverse_sigmoide(x, valeur_sup=1.0, valeur_inf=.0, pente=1):\n",
    "    \"\"\"Personnalisable inverse sigmoide.\n",
    "\n",
    "    Arguments d'entrées:\n",
    "        x (float)\n",
    "        valeur_sup, valeur_inf, pente (float): paramètres\n",
    "    \n",
    "    Arguments de sorties:\n",
    "        (float)\n",
    "    \"\"\"\n",
    "    return math.log( ( x - valeur_inf ) / ( valeur_sup - valeur_inf) ) / pente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:54:54.210784Z",
     "start_time": "2020-03-24T14:54:53.998743Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import operator\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:54:54.232807Z",
     "start_time": "2020-03-24T14:54:54.216462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 887 entries, 0 to 886\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Survived                 887 non-null    int64  \n",
      " 1   Pclass                   887 non-null    int64  \n",
      " 2   Name                     887 non-null    object \n",
      " 3   Sex                      887 non-null    object \n",
      " 4   Age                      887 non-null    float64\n",
      " 5   Siblings/Spouses Aboard  887 non-null    int64  \n",
      " 6   Parents/Children Aboard  887 non-null    int64  \n",
      " 7   Fare                     887 non-null    float64\n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 55.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"titanic.csv\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:54:54.248545Z",
     "start_time": "2020-03-24T14:54:54.241588Z"
    }
   },
   "outputs": [],
   "source": [
    "age = dataset.Age\n",
    "fare = dataset.Fare\n",
    "survived = dataset.Survived\n",
    "sex = dataset.Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:54:54.275452Z",
     "start_time": "2020-03-24T14:54:54.253428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survived': <Nature.BINARY: 'Binary'>,\n",
       " 'Pclass': <Nature.CATEGORY: 'Category'>,\n",
       " 'Name': <Nature.CATEGORY: 'Category'>,\n",
       " 'Sex': <Nature.BINARY: 'Binary'>,\n",
       " 'Age': <Nature.NUMBER: 'Number'>,\n",
       " 'Siblings/Spouses Aboard': <Nature.NUMBER: 'Number'>,\n",
       " 'Parents/Children Aboard': <Nature.NUMBER: 'Number'>,\n",
       " 'Fare': <Nature.NUMBER: 'Number'>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ k: trouver_type(dataset[k], SEUIL_CATEGORIE_PAR_DEFAUT) for k in dataset }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T16:00:48.109673Z",
     "start_time": "2020-03-24T16:00:48.101070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2020-08-01\n",
       "1     2020-08-02\n",
       "2     2020-08-03\n",
       "3     2020-08-04\n",
       "4     2020-08-05\n",
       "5     2020-08-06\n",
       "6     2020-08-07\n",
       "7     2020-08-08\n",
       "8     2020-08-09\n",
       "9     2020-08-10\n",
       "10    2020-08-11\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.Series([dt.date(2020, 8, i) for i in range(1, 12)])\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T16:35:03.413390Z",
     "start_time": "2020-03-24T16:35:03.407216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.dtype == np.object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T16:16:31.787831Z",
     "start_time": "2020-03-24T16:16:31.774485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.to_datetime(series).dtype\n",
    "ndate = lambda y: any( y == t for t in (np.dtype('datetime64'), np.dtype('datetime64[s]'), np.dtype('datetime64[ns]')) )\n",
    "ndate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T16:16:46.810477Z",
     "start_time": "2020-03-24T16:16:46.786583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('<M8[ns]'), True, False, False, True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type_variable_datetime = variable.dtype == datetime64 or any(map(lambda s: is_date(s, fuzzy=fuzzy, language=language, kwargs=kwargs), variable))\n",
    "y = pd.to_datetime(series).dtype\n",
    "z = ndate(y)\n",
    "a = y == np.datetime64\n",
    "b = is_date(series[0], fuzzy=False, language=None, kwargs=None)\n",
    "c = isinstance(series[0], (dt.timedelta, dt.date))\n",
    "y, z, a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T15:02:34.285988Z",
     "start_time": "2020-03-24T15:02:34.279443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Nature.CATEGORY: 'Category'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trouver_type(series, SEUIL_CATEGORIE_PAR_DEFAUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:39:20.125986Z",
     "start_time": "2020-03-24T14:39:19.826366Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-05aeebb46196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "    Fonctions permettant de calculer les différentes corrélations\n",
    "    entre les variables d'un même jeu de données.\n",
    "    Un vecteur peut être une liste, un Pandas.Series ou NumPy.array. \n",
    "\"\"\"\n",
    "import math\n",
    "import statistics as stat\n",
    "import operator\n",
    "\n",
    "from scipy import stats\n",
    "from numpy import datetime64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from .utils import *\n",
    "\n",
    "\n",
    "def calculer_covariance(variable_1, variable_2):\n",
    "    \"\"\"Calcul de la covariance empirique entre deux variables quantitatives.\n",
    "\n",
    "    Arguments d'entrée:\n",
    "        variable_1, variable_2 (numpy.array)\n",
    "\n",
    "    Arguments de sortie:\n",
    "        (float)\n",
    "    \"\"\"\n",
    "    n = len(variable_1)\n",
    "    moyenne_variable_1 = stat.mean(variable_1)\n",
    "    moyenne_variable_2 = stat.mean(variable_2)\n",
    "    return 0 if n == 0 else sum(x_i_1 * x_i_2 for x_i_1, x_i_2 in zip(variable_1 - moyenne_variable_1, variable_2 - moyenne_variable_2)) / n\n",
    "\n",
    "\n",
    "def calculer_correlation_pearson(variable_1, variable_2):\n",
    "    \"\"\"Calcul de la correlation entre deux variables quantitatives.\n",
    "    On retourne donc une valeur entrez 0 et 1.\n",
    "\n",
    "    Arguments d'entrée:\n",
    "        variable_1, variable_2 (numpy.array)\n",
    "\n",
    "    Arguments de sortie:\n",
    "        (float)\n",
    "    \"\"\"\n",
    "    correlation = calculer_covariance(variable_1, variable_2)\n",
    "    variance_1 = calculer_covariance(variable_1, variable_1)\n",
    "    variance_2 = calculer_covariance(variable_2, variable_2)\n",
    "\n",
    "    if variance_1 == 0 or variance_2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return correlation / (math.sqrt(variance_1) * math.sqrt(variance_2))\n",
    "\n",
    "\n",
    "def test_correlation_pearson(variable_1, variable_2, covariance=None):\n",
    "    \"\"\"Test de corrélation de Pearson\n",
    "    pour une corrélation entre deux variables quantitatives.\n",
    "\n",
    "    Arguments d'entrées:\n",
    "        variable_1, variable_2 (NumPy.array)\n",
    "        covariance (Python function): function pour calculer une covariance entre deux variables.\n",
    "\n",
    "    Arguments de sorties:\n",
    "        statistique, p_valeur, coefficient (float)\n",
    "    \"\"\"\n",
    "    if covariance is None:\n",
    "        covariance = calculer_covariance\n",
    "\n",
    "    # On supprime les valeurs nulles si nécessaire\n",
    "    condition = ~( np.isnan(variable_1) | np.isnan(variable_2) )\n",
    "    variable_1, variable_2 = variable_1[condition], variable_2[condition]\n",
    "\n",
    "    n = len(variable_1)\n",
    "\n",
    "    # Calcul du coefficient\n",
    "    coefficient = calculer_correlation_pearson(variable_1, variable_2)\n",
    "\n",
    "    # Calcul de la statistique\n",
    "    statistique = np.inf\n",
    "    if coefficient != 1.0:\n",
    "        statistique = coefficient / math.sqrt((1 - coefficient ** 2)/(n - 2))\n",
    "\n",
    "    # Calcul de la p_valeur\n",
    "    p_valeur = (1 - stats.t.cdf(x=statistique, df=n - 2))\n",
    "\n",
    "    return statistique, p_valeur, coefficient\n",
    "\n",
    "\n",
    "def test_correlation_eta_squared(variable_quantitative, variable_qualitative):\n",
    "    \"\"\"Test de corrélation de eta squared\n",
    "    pour une corrélation entre une variable quantitative et une variable qualitative.\n",
    "\n",
    "    Arguments d'entrées:\n",
    "        variable_quantitative, variable_qualitative (NumPy.array)\n",
    "\n",
    "    Arguments de sorties:\n",
    "        statistique, p_valeur, coefficient (float)\n",
    "    \"\"\"\n",
    "    # On supprime les valeurs nulles si nécessaire\n",
    "    variable_qualitative = variable_qualitative.astype(str)\n",
    "\n",
    "    condition = ~( np.isnan(variable_quantitative) | np.isnan([ valeur_numpy_nulle(x) for x in variable_qualitative ]) )\n",
    "    variable_quantitative = variable_quantitative[condition]\n",
    "    variable_qualitative = variable_qualitative[condition]\n",
    "\n",
    "    n_quantitative = len(variable_quantitative)\n",
    "    classes = np.unique(variable_qualitative)\n",
    "    n_qualitative = len(classes)\n",
    "\n",
    "    # Calcul du coefficient\n",
    "    moyenne_quantitative = stat.mean(variable_quantitative)\n",
    "    classes = [ variable_quantitative[variable_qualitative == cl] for cl in classes ]\n",
    "    classes = [(len(vecteur), stat.mean(vecteur)) for vecteur in classes]\n",
    "    coefficient_sct = sum((valeur - moyenne_quantitative)\n",
    "                          ** 2 for valeur in variable_quantitative)\n",
    "    coefficient_sce = sum(nombre_classe_i * (moyenne_classe_i - moyenne_quantitative)\n",
    "                          ** 2 for nombre_classe_i, moyenne_classe_i in classes)\n",
    "\n",
    "    coefficient = coefficient_sce / coefficient_sct if coefficient_sct != 0 else np.inf\n",
    "\n",
    "    # Calcul de la statistique\n",
    "    statistique = (coefficient * (n_quantitative - n_qualitative)\n",
    "                   ) / ((1 - coefficient) * (n_qualitative - 1))\n",
    "\n",
    "    # Calcul de la p_valeur\n",
    "    p_valeur = 1 - stats.f.cdf(statistique, n_quantitative -\n",
    "                              n_qualitative, n_qualitative - 1)\n",
    "\n",
    "    return statistique, p_valeur, coefficient\n",
    "\n",
    "\n",
    "def calculer_tableau_contingence(variable_1, variable_2, nom_1=None, nom_2=None, rajouter_colonne_total=False):\n",
    "    \"\"\" Calcul du tableau de contingence entre deux variables qualitatives.\n",
    "    Basé sur https://openclassrooms.com/fr/courses/4525266-decrivez-et-nettoyez-votre-jeu-de-donnees/4775616-analysez-deux-variables-qualitatives-avec-le-chi-2\n",
    "\n",
    "\n",
    "    Arguments d'entrées:\n",
    "        variable_1, variable_2 (numpy.array)\n",
    "        nom_1, nom_2 (str)\n",
    "\n",
    "    Arguments de sortie:\n",
    "        tableau_de_contingence (NumPy.array) \n",
    "    \"\"\"\n",
    "    if nom_1 is None:\n",
    "        nom_1 = 'variable_1'\n",
    "    \n",
    "    if nom_2 is None:\n",
    "        nom_2 = 'variable_2'\n",
    "    \n",
    "    \n",
    "    nom_total_1, nom_total_2 = 'total_1', 'total_2'\n",
    "    tableau_de_contingence = (pd.DataFrame(data={nom_1: variable_1, nom_2: variable_2})\n",
    "                              .pivot_table(index=nom_1, columns=nom_2, aggfunc=len))\n",
    "\n",
    "    if rajouter_colonne_total:\n",
    "        tableau_de_contingence.loc[:,\n",
    "                                   nom_total_2] = tableau_de_contingence[nom_1].value_counts()\n",
    "        tableau_de_contingence.loc[nom_total_1,\n",
    "                                   :] = tableau_de_contingence[nom_2].value_counts()\n",
    "        tableau_de_contingence.loc[nom_total_1,\n",
    "                                   nom_total_2] = len(variable_1)\n",
    "\n",
    "    return tableau_de_contingence.fillna(0).values\n",
    "\n",
    "\n",
    "def test_correlation_chi_squared(variable_1, variable_2, nom_1=None, nom_2=None, contingence=None, **kwargs):\n",
    "    \"\"\"Test de corrélation du chi-deux\n",
    "    pour une corrélation entre deux variables qualtitatives.\n",
    "\n",
    "    Arguments d'entrées:\n",
    "        variable_1, variable_2 (NumPy.array)\n",
    "        nom_1, nom_2 (str)\n",
    "        contingence (Python function): function pour calculer un tableau de contingence entre deux variables.\n",
    "\n",
    "    Arguments de sorties:\n",
    "        statistique, p_valeur, coefficient (float): coefficient basé sur le coefficient de Cramer \n",
    "    \"\"\"\n",
    "    if contingence is None:\n",
    "        contingence = lambda var_1, var_2: calculer_tableau_contingence(variable_1=var_1,\n",
    "                                                                        variable_2=var_2,\n",
    "                                                                        nom_1=nom_1,\n",
    "                                                                        nom_2=nom_2,\n",
    "                                                                        rajouter_colonne_total=False)\n",
    "\n",
    "    # On supprime les valeurs nulles si nécessaire\n",
    "    variable_1 = variable_1.astype(str)\n",
    "    variable_2 = variable_2.astype(str)\n",
    "    condition = ~( np.isnan([ valeur_numpy_nulle(x) for x in variable_1]) | np.isnan([ valeur_numpy_nulle(x) for x in variable_2]) )\n",
    "    variable_1 = variable_1[condition]\n",
    "    variable_2 = variable_2[condition]\n",
    "\n",
    "    n = len(variable_1)\n",
    "    tableau_de_contingence = contingence(variable_1, variable_2)\n",
    "    total_1 = pd.Series(variable_1).value_counts()\n",
    "    total_2 = pd.Series(variable_2).value_counts()\n",
    "\n",
    "    terme_independance = sum( t_1 * t_2 for t_1, t_2 in zip(total_1, total_2) ) / n\n",
    "\n",
    "    # Calcul de la statistique\n",
    "    statistique = ( (tableau_de_contingence - terme_independance) ** 2 / terme_independance ).tolist()\n",
    "    statistique = sum( sum(ligne) for ligne in statistique )              \n",
    "\n",
    "    # Calcul du coefficient\n",
    "    coefficient = math.sqrt( statistique / (n * (min(len(total_1), len(total_2)) - 1)) )\n",
    "\n",
    "    # Calcul de la p_valeur\n",
    "    p_valeur = 1 - stats.chi2.cdf(x=statistique, df=(len(total_1) - 1) * (len(total_2) - 1))\n",
    "\n",
    "    return statistique, p_valeur, coefficient\n",
    "\n",
    "\n",
    "def test_de_correlation(variable_1, variable_2, nom_1=None, nom_2=None, seuil_categorie=None, *args, **kwargs):\n",
    "    \"\"\"Pour calculer la correlation variables.\n",
    "\n",
    "    Arguments d'entrée:\n",
    "        variable_1, variable_2 (pandas.Series)\n",
    "        nom_1, nom_2 (str)\n",
    "\n",
    "    Arguments de sortie:\n",
    "        statistique, p_valeur, coefficient (float)\n",
    "    \"\"\"\n",
    "    variable_1_type = donner_type(variable=variable_1, seuil_categorie=seuil_categorie, type_attitre=kwargs.get(nom_1, None))\n",
    "    variable_2_type = donner_type(variable=variable_2, seuil_categorie=seuil_categorie, type_attitre=kwargs.get(nom_2, None))\n",
    "\n",
    "\n",
    "    if (variable_1_type, variable_2_type) == (Nature.NUMBER, Nature.NUMBER):\n",
    "        covariance = kwargs.get('covariance', None)\n",
    "        return test_correlation_pearson(variable_1, variable_2, covariance)\n",
    "    elif variable_1_type == Nature.NUMBER:\n",
    "        return test_correlation_eta_squared(variable_1, variable_2)\n",
    "    elif variable_2_type == Nature.NUMBER:\n",
    "        return test_correlation_eta_squared(variable_2, variable_1)\n",
    "    else:\n",
    "        contingence = kwargs.get('contingence', None)\n",
    "        return test_correlation_chi_squared(variable_1=variable_1,\n",
    "                                            variable_2=variable_2,\n",
    "                                            nom_1=nom_1,\n",
    "                                            nom_2=nom_2,\n",
    "                                            contingence=contingence\n",
    "                                            ) \n",
    "\n",
    "\n",
    "def matrice_de_correlation(tableau, format_compact=True, calculer_autocorrelation=False, retourner_liste=False, *args, **kwargs):\n",
    "    \"\"\"Pour calculer la matrice de correlation d'un jeu de données.\n",
    "\n",
    "    Arguments d'entrée:\n",
    "        tableau (Pandas.DataFrame)\n",
    "        format_compact (bool): Si Vrai, matrice de triplet sinon trois matrices en sortie\n",
    "        calculer_autocorrelation (bool): Si Vrai, calculer l'autocorrelation qui doit s'annuler\n",
    "            #TO FIX à supprimer, peut-être inutile\n",
    "        retourner_liste (bool): Si vrai retourner une liste\n",
    "            #TO FIX à supprimer, peut-être inutile\n",
    "\n",
    "    Arguments de sortie:\n",
    "        correlation or correlation_statistique, correlation_p_valeur, correlation_coefficient (NumPy.array or list)\n",
    "    \"\"\"\n",
    "    nom_de_variables = list(tableau)\n",
    "    correlation = []\n",
    "    for variable_1 in nom_de_variables:\n",
    "        correlation_pour_variable_1 = []\n",
    "        for variable_2 in nom_de_variables:\n",
    "            #print()\n",
    "            #print(variable_1, \" vs \", variable_2)\n",
    "            triplet = (np.nan, np.nan, np.nan)\n",
    "            if not calculer_autocorrelation and variable_1 == variable_2:\n",
    "                triplet = (1, np.nan, 1)\n",
    "            else:\n",
    "                triplet = test_de_correlation(tableau[variable_1].values,\n",
    "                                              tableau[variable_2].values,\n",
    "                                              *args,\n",
    "                                              **kwargs)\n",
    "            correlation_pour_variable_1.append(triplet)\n",
    "            #print(\"***********\")\n",
    "        correlation.append(correlation_pour_variable_1)\n",
    "\n",
    "    if format_compact:\n",
    "        if not retourner_liste:\n",
    "            correlation = np.array(correlation)\n",
    "        return correlation\n",
    "    else:\n",
    "        correlation_statistique = [list(map(operator.itemgetter(0), ligne)) for ligne in correlation]\n",
    "        correlation_p_valeur = [list(map(operator.itemgetter(1), ligne)) for ligne in correlation]\n",
    "        correlation_coefficient = [list(map(operator.itemgetter(2), ligne)) for ligne in correlation]\n",
    "\n",
    "        if not retourner_liste:\n",
    "            correlation_statistique = np.array(correlation_statistique)\n",
    "            correlation_p_valeur = np.array(correlation_p_valeur)\n",
    "            correlation_coefficient = np.array(correlation_coefficient)\n",
    "        return correlation_statistique, correlation_p_valeur, correlation_coefficient\n",
    "\n",
    "\n",
    "def normaliser_significativite_booleen(p_valeur, alpha, tolerance, *args, **kwargs):\n",
    "    \"\"\"Retourne une valeur booléenne pour qualifier la significativité d'un test statistique.\n",
    "\n",
    "    Arguments d'entrée:\n",
    "        p_valeur, alpha, tolerance (float)\n",
    "\n",
    "\n",
    "    Arguments de sortie:\n",
    "        significativite (bool)\n",
    "    \"\"\"\n",
    "    significativite = p_valeur >= alpha\n",
    "    return significativite\n",
    "\n",
    "\n",
    "def normaliser_significativite_numerique(p_valeur, alpha, tolerance, *args, **kwargs):\n",
    "    \"\"\"Retourne une valeur entre 0 et 1 pour quantifier la significativité d'un test statistique.\n",
    "\n",
    "    Arguments d'entrée:\n",
    "        p_valeur, alpha, tolerance (float)\n",
    "\n",
    "\n",
    "    Arguments de sortie:\n",
    "        significativite (float)\n",
    "    \"\"\"\n",
    "    # Première normalisation entre 0 et 1 \n",
    "    significativite = max(0, (p_valeur - alpha) / (1 - alpha))\n",
    "\n",
    "    # Seconde normalisation pour changer la répartition des valeurs normalisées\n",
    "    significativite = sigmoide(x=significativite,\n",
    "                               valeur_sup=kwargs.get('valeur_sup',1.0),\n",
    "                               valeur_inf=kwargs.get('valeur_inf',.0),\n",
    "                               pente=kwargs.get('pente',1))\n",
    "    return significativite\n",
    "\n",
    "\n",
    "def matrice_de_significativite(tableau, alpha, retourner_booleen=False, tolerance=.01, *args, **kwargs):\n",
    "    \"\"\"Retourne une matrice où chaque valeur sera comprise entre 0 et 1 ou sera booléenne.\n",
    "\n",
    "    Arguments d'entrée:\n",
    "            correlation_p_valeur (Numpy.array or list)\n",
    "            alpha (float)\n",
    "            retourner_booleen (bool)\n",
    "            tolerance (float)\n",
    "\n",
    "\n",
    "        Arguments de sortie:\n",
    "            tableau_de_taille (Numpy.array)\n",
    "    \"\"\"\n",
    "    to_array = isinstance(tableau, np.ndarray)\n",
    "    if to_array:\n",
    "        tableau = tableau.tolist()\n",
    "    \n",
    "    normaliser_significativite = None\n",
    "    if retourner_booleen:\n",
    "        normaliser_significativite = lambda s: normaliser_significativite_booleen(p_valeur=s,\n",
    "                                                                                  alpha=alpha,\n",
    "                                                                                  tolerance=tolerance,\n",
    "                                                                                  *args, \n",
    "                                                                                  **kwargs)\n",
    "    else:\n",
    "        normaliser_kwargs = { 'valeur_sup': 1.0,\n",
    "                              'valeur_inf': tolerance,\n",
    "                              'pente': 5\n",
    "                            }\n",
    "        normaliser_significativite = lambda s: normaliser_significativite_numerique(p_valeur=s,\n",
    "                                                                                    alpha=alpha,\n",
    "                                                                                    tolerance=tolerance,\n",
    "                                                                                    *args, \n",
    "                                                                                    **{**kwargs, **normaliser_kwargs})\n",
    "\n",
    "    tableau_de_taille = [list(map(normaliser_significativite, ligne)) for ligne in tableau]\n",
    "\n",
    "    if to_array:\n",
    "        return np.array(tableau_de_taille)\n",
    "    return tableau_de_taille\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:37:59.452686Z",
     "start_time": "2020-03-24T14:37:59.449946Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"Module centré sur le preprocessing d'un jeu de données.\n",
    "Bien que basé sur un projet d'analyse de données spécifique (OpenFoodFacts),\n",
    "le contenu a pour vocation d'être générique.\"\"\"\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def eliminer_colonne_vide(tableau, taux_de_vide_minimum=0.50, retourner_details=True, chemin_fichier='barh_classement_colonnes_vides.png'):\n",
    "    \"\"\"Supprimer les colonnes avec un taux de remplissage inférieur à une référence donnée.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        tableau (pandas.DataFrame)\n",
    "        taux_de_vide_minimum (float): au delà de ce taux, la colonne sera considérée vide\n",
    "        retourner_details (bool): si True, renvoie le classement des colonnes\n",
    "            sur leurs taux de remplissage.\n",
    "    \n",
    "    Arguments de sortie:\n",
    "        tableau_nettoye (pandas.DataFrame)\n",
    "        classement (pandas.DataFrame)\n",
    "    \"\"\"\n",
    "    classement = pd.DataFrame()\n",
    "    classement['colonne'] = list(tableau)\n",
    "    classement['taux_de_vide'] = (\n",
    "        classement['colonne']\n",
    "        .apply(lambda colonne: tableau[colonne].isna().mean())\n",
    "    )\n",
    "    classement = classement.sort_values('taux_de_vide', ascending=False)\n",
    "    print(\"clst df:\\n \", classement)\n",
    "    \n",
    "    colonnes_a_eliminer = classement.loc[classement['taux_de_vide']>=taux_de_vide_minimum, 'colonne']\n",
    "    tableau_nettoye = tableau.drop(columns=colonnes_a_eliminer)\n",
    "\n",
    "    if retourner_details:\n",
    "        # Initialise plot\n",
    "        ax = classement.set_index('colonne').plot(kind='barh', figsize=(8, 10), color='#86bf91', zorder=2, width=0.85)\n",
    "\n",
    "        # Despine -- remove figure's borders\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "        # Set x-axis\n",
    "        ax.set_xlabel(\"Ratio d'éléments vides\", labelpad=20, weight='bold', size=12)\n",
    "        # il faudra voir\n",
    "\n",
    "        # Set y-axis\n",
    "        ax.set_ylabel(\"Colonnes\", labelpad=20, weight='bold', size=12)\n",
    "\n",
    "        # Add a legend\n",
    "        ax.legend(ncol=2, loc=\"upper right\")\n",
    "\n",
    "\n",
    "        # Save figure\n",
    "        (ax\n",
    "        .get_figure()\n",
    "        .savefig(chemin_fichier, bbox_inches=\"tight\")\n",
    "        )\n",
    "\n",
    "        return tableau_nettoye, classement\n",
    "    \n",
    "    return tableau_nettoye\n",
    "\n",
    "\n",
    "def eliminer_ligne_vide(tableau, taux=None, retourner_details=True):\n",
    "    \"\"\"Supprimer les lignes avec un taux de remplissage inférieur à une référence donnée.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        tableau (pandas.DataFrame)\n",
    "        taux (float)\n",
    "        retourner_details (bool): si True, renvoie le classement des colonnes\n",
    "            sur leurs taux de remplissage.\n",
    "    \n",
    "    Arguments de sorties:\n",
    "        tableau_nettoye (pandas.DataFrame)\n",
    "        classement (pandas.DataFrame)\n",
    "    \"\"\"\n",
    "    if not taux:\n",
    "        taux = 0.50\n",
    "    taux = len(tableau.columns) * taux // 1\n",
    "\n",
    "    lignes_a_eliminer = tableau.isna().sum(1)\n",
    "    tableau_nettoye = tableau[ lignes_a_eliminer>=taux ]\n",
    "\n",
    "    if retourner_details:\n",
    "        pass\n",
    "        # (classement\n",
    "        # .plot(kind='barh')\n",
    "        # .get_figure()\n",
    "        # .savefig(chemin_fichier)\n",
    "        # )\n",
    "    return tableau_nettoye\n",
    "\n",
    "\n",
    "def trouver_valeur_par_défaut(valeur_par_defaut, clef):\n",
    "    \"\"\"Permet de personnaliser une valeur par défaut suivant un axe.\"\"\"\n",
    "    if isinstance(valeur_par_defaut, dict):\n",
    "        return valeur_par_defaut.get(clef, None)\n",
    "    return valeur_par_defaut\n",
    "\n",
    "\n",
    "def premiere_occurence(vecteur, valeur_par_defaut=None):\n",
    "    \"\"\"Wrapper de la méthode first_valid_index d'un Pandas.Series.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        vecteur (pandas.Series)\n",
    "        valeur_par_defaut\n",
    "    Arguments de sortie:\n",
    "        (Python Object)\n",
    "    \"\"\"\n",
    "    index = vecteur.first_valid_index()\n",
    "    if index is None:\n",
    "        return valeur_par_defaut\n",
    "    return vecteur[index]\n",
    "\n",
    "\n",
    "def plus_frequente_occurence(vecteur, valeur_par_defaut=None):\n",
    "    \"\"\"Wrapper de la méthode first_valid_index d'un Pandas.Series.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        vecteur (pandas.Series)\n",
    "        valeur_par_defaut\n",
    "    Arguments de sortie:\n",
    "        (Python Object)\n",
    "    \"\"\"\n",
    "    return vecteur.value_counts(ascending=False, dropna=False).iloc[0]\n",
    "\n",
    "\n",
    "def calculer_imputation(tableau, strategie='première occurence', valeur_par_defaut=None):\n",
    "    \"\"\"Imputer suivant la strategie choisie.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        tableau (pandas.DataFrame)\n",
    "        strategie (str):\n",
    "            'première occurence' retourne la première valeur non nulle,\n",
    "            'la plus fréquente' retourne la valeur la plus fréquente non nulle\n",
    "    \n",
    "    Arguments de sortie:\n",
    "        tableau_nettoye (pandas.DataFrame)\n",
    "    \"\"\"\n",
    "    fonction = None\n",
    "    if strategie ==\"première occurence\":\n",
    "        fonction = premiere_occurence\n",
    "    elif strategie==\"la plus fréquente\":\n",
    "        fonction = plus_frequente_occurence\n",
    "    else:\n",
    "        raise ValueError(f\"l'argument strategie ne peut prendre que les valeurs suivantes: 'première occurence' et 'la plus fréquente'. La valeur donnée ici est {strategie}\")\n",
    "    if tableau.empty:\n",
    "        raise ValueError(\"l'argument tableau est vide.\")\n",
    "    tableau_impute = {index: fonction(tableau[index], trouver_valeur_par_défaut(valeur_par_defaut, index)) for index in tableau.columns}\n",
    "    tableau_impute = pd.Series(tableau_impute)\n",
    "    return tableau_impute\n",
    "\n",
    "\n",
    "def eliminer_doublons(tableau, colonnes_ciblees=None, strategie='première occurence'):\n",
    "    \"\"\"Retourne un tableau avec des individus uniques\n",
    "    basés sur les colonnes ciblées.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        tableau (pandas.DataFrame)\n",
    "        colonnes_ciblees (list)\n",
    "        strategie (str): méthode pour trouver l'unique individu\n",
    "    \n",
    "    Arguments de sortie:\n",
    "        tableau_nettoye (pandas.DataFrame)\n",
    "    \"\"\"\n",
    "    if colonnes_ciblees is None or not isinstance(colonnes_ciblees, list):\n",
    "        if not isinstance(colonnes_ciblees, list):\n",
    "            print(f\"l'argument colonnes_ciblees n'est pas de type list mais {type(colonnes_ciblees)}\")\n",
    "            return tableau.drop_duplicates()\n",
    "    methode = lambda tableau: calculer_imputation(tableau, strategie)\n",
    "    return (\n",
    "        tableau\n",
    "        .groupby(colonnes_ciblees)\n",
    "        .agg(methode)\n",
    "        .reset_index(drop=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:38:37.845789Z",
     "start_time": "2020-03-24T14:38:35.572333Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"Module centré sur le nettoyage et l'analyse exploratoire d'un jeu de données.\n",
    "Bien que basé sur un projet d'analyse de données spécifique (OpenFoodFacts),\n",
    "le contenu a pour vocation d'être générale.\"\"\"\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "def dessiner_graphique_barres(tableau, colonne, fichier, valeur_manquante='valeur manquante', titre=None, rotation=None):\n",
    "    \"\"\"Implémenter un histogramme pour une variable qualitative.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        tableau (pandas.DataFrame)\n",
    "        colonne (str)\n",
    "        fichier (str): chemin relatif du graphique\n",
    "        valeur_manquante (str): alias pour les np.nan\n",
    "        titre (str)\n",
    "        rotation (int)\n",
    "    \"\"\"\n",
    "    if titre is None:\n",
    "        titre = f\"Histogramme de la variable qualitative {colonne}\"\n",
    "    denombrement = (\n",
    "        tableau\n",
    "        [colonne]\n",
    "        .value_counts(normalize=True, dropna=False)\n",
    "        .mul(100)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': colonne, colonne: 'pourcentage'})\n",
    "        .fillna(valeur_manquante)\n",
    "    )\n",
    "    barres = sns.barplot(x=colonne, y=\"pourcentage\", data=denombrement)\n",
    "    for barre in barres.patches:\n",
    "        barres.annotate('{:.2f}%'.format(barre.get_height()), (barre.get_x()+0.5, barre.get_height()+1))\n",
    "    plt.title(titre)\n",
    "    plt.xlabel(colonne)\n",
    "    plt.ylabel('pourcentage [%]')\n",
    "    if rotation:\n",
    "        plt.setp(barres.get_xticklabels(), rotation=rotation)\n",
    "    (barres\n",
    "    .get_figure()\n",
    "    .savefig(fichier, bbox_inches=\"tight\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def dessiner_distributions_univariees(tableau, nbr_colonnes, fichier, titre=None, dropna=True):\n",
    "    \"\"\"Implémenter un histogramme ou une distribution pour chaque variable.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        tableau (pandas.DataFrame)\n",
    "        nbr_colonne (int): on aura une table de graphique de nbr_colonnes colonnes\n",
    "        fichier (str): chemin relatif du graphique\n",
    "        titre (str)\n",
    "    \"\"\"\n",
    "    if titre is None:\n",
    "        titre = f\"Distributions univariées\"\n",
    "    nbr_graphiques = len(tableau.columns)\n",
    "    fig = plt.figure(figsize=(nbr_graphiques, nbr_graphiques))\n",
    "    for index, colonne in enumerate(tableau.columns):\n",
    "        vecteur = tableau[colonne]\n",
    "        if dropna:\n",
    "            vecteur = vecteur.dropna()\n",
    "        axe = fig.add_subplot((nbr_graphiques // nbr_colonnes) + 1,\n",
    "                            nbr_colonnes,\n",
    "                            index + 1)\n",
    "        sns.distplot(vecteur,\n",
    "                    ax=axe,\n",
    "                    hist=False,\n",
    "                    #rug=True,\n",
    "                    kde_kws={\"kernel\": \"gau\"}\n",
    "        )\n",
    "    plt.axis('tight')\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().savefig(fichier, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def dessiner_graphique_donut(tableau, colonne, fichier, valeur_manquante='valeur manquante', titre=None):\n",
    "    \"\"\"Implémenter un donut pour une variable qualitative.\n",
    "    \n",
    "    Arguments d'entrée:\n",
    "        tableau (pandas.DataFrame)\n",
    "        colonne (str)\n",
    "        fichier (str): chemin relatif du graphique\n",
    "        valeur_manquante (str): alias pour les np.nan\n",
    "        titre (str)\n",
    "    \"\"\"\n",
    "    if titre is None:\n",
    "        titre = f\"Répartition de la variable qualitative {colonne}\"\n",
    "    camembert = (\n",
    "        tableau\n",
    "        [colonne]\n",
    "        .value_counts(normalize=True, dropna=False)\n",
    "        .mul(100)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': colonne, colonne: 'pourcentage'})\n",
    "        .fillna(valeur_manquante)\n",
    "    )\n",
    "    nbr_categories, _ = camembert.shape\n",
    "    if colormap is None:\n",
    "        #colormap = plt.get_cmap('tab20')\n",
    "        colormap = sns.color_palette(\"husl\", nbr_categories)\n",
    "    # pour isoler la première part\n",
    "    explode = [0.1] + [0] * (nbr_categories - 1)\n",
    "    labels = [\"{}: {:.2f}%\".format(vecteur[colonne], vecteur['pourcentage']) for _, vecteur in camembert.iterrows()]\n",
    "    plt.pie(camembert['pourcentage'].mul(1/100),\n",
    "            labels=labels,\n",
    "            colors=colormap,\n",
    "            explode=explode,\n",
    "        )\n",
    "    trou_du_donut = plt.Circle(xy=(0, 0), radius=0.47, facecolor='white')\n",
    "    plt.gcf().gca().add_artist(trou_du_donut)\n",
    "    plt.axis('equal')\n",
    "    plt.gcf().savefig(fichier, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "#f, ax = plt.subplots(figsize=(22,5))\n",
    "#cols = [x for x in df]\n",
    "#idx = cols.index('nutrition-score-fr_100g')\n",
    "#sns.heatmap(corr_mat[idx, :].reshape(1, -1),\n",
    "#            yticklabels='',\n",
    "#            xticklabels=cols, square=True, linewidths=1, linecolor='white',\n",
    "#            cbar=True, cbar_kws={\"shrink\": .45},\n",
    "#            annot=True, fmt='.3f',\n",
    "#            cmap=\"BuPu\"\n",
    "#           );\n",
    "#plt.title(\"Corrélation de la variable 'nutrition-score-fr_100g'\");\n",
    "#plt.gcf().savefig('graphs/my_heatmap_score_first.png', bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:38:58.678187Z",
     "start_time": "2020-03-24T14:38:58.589682Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Functions to visualize matrices of data.\n",
    "    It is a custom version of a Heatmap allowing\n",
    "    cells size's customization.\n",
    "    It is based on matrix.py in https://github.com/mwaskom/seaborn\n",
    "    ( commit 065d3c1 ) by Michael L. Waskom .\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "#import itertools\n",
    "import functools\n",
    "#import datetime\n",
    "import numbers\n",
    "import operator\n",
    "\n",
    "import matplotlib as mpl\n",
    "#from matplotlib.collections import LineCollection\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib import gridspec\n",
    "#import matplotlib.patheffects as patheffects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from scipy.cluster import hierarchy\n",
    "\n",
    "import seaborn as sns\n",
    "from seaborn import cm\n",
    "#from seaborn.axisgrid import Grid\n",
    "from seaborn.utils import (\n",
    "    despine, axis_ticklabels_overlap, relative_luminance, to_utf8)\n",
    "from seaborn.external.six import string_types\n",
    "\n",
    "__all__  = [ 'custom_cells_heatmap' ]\n",
    "\n",
    "\n",
    "def _replace_bool(value):\n",
    "    \"\"\"NEW-- Custom function to replace boolean values\"\"\"\n",
    "    if not isinstance(value, bool):\n",
    "        return value\n",
    "    return 1.0 if value else 0.0\n",
    "\n",
    "\n",
    "def _index_to_label(index):\n",
    "    \"\"\"Convert a pandas index or multiindex to an axis label.\"\"\"\n",
    "    if isinstance(index, pd.MultiIndex):\n",
    "        return \"-\".join(map(to_utf8, index.names))\n",
    "    else:\n",
    "        return index.name\n",
    "\n",
    "\n",
    "def _index_to_ticklabels(index):\n",
    "    \"\"\"Convert a pandas index or multiindex into ticklabels.\"\"\"\n",
    "    if isinstance(index, pd.MultiIndex):\n",
    "        return [\"-\".join(map(to_utf8, i)) for i in index.values]\n",
    "    else:\n",
    "        return index.values\n",
    "\n",
    "\n",
    "def _convert_colors(colors):\n",
    "    \"\"\"Convert either a list of colors or nested lists of colors to RGB.\"\"\"\n",
    "    to_rgb = mpl.colors.colorConverter.to_rgb\n",
    "\n",
    "    if isinstance(colors, pd.DataFrame):\n",
    "        # Convert dataframe\n",
    "        return pd.DataFrame({col: colors[col].map(to_rgb)\n",
    "                             for col in colors})\n",
    "    elif isinstance(colors, pd.Series):\n",
    "        return colors.map(to_rgb)\n",
    "    else:\n",
    "        try:\n",
    "            to_rgb(colors[0])\n",
    "            # If this works, there is only one level of colors\n",
    "            return list(map(to_rgb, colors))\n",
    "        except ValueError:\n",
    "            # If we get here, we have nested lists\n",
    "            return [list(map(to_rgb, l)) for l in colors]\n",
    "\n",
    "\n",
    "def _matrix_mask(data, mask):\n",
    "    \"\"\"\n",
    "        Ensure that data and mask are compatible and add missing values.\n",
    "        Values will be plotted for cells where ``mask`` is ``False``.\n",
    "        ``data`` is expected to be a DataFrame; ``mask`` can be an array or\n",
    "        a DataFrame.\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = np.zeros(data.shape, np.bool)\n",
    "\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        # For array masks, ensure that shape matches data then convert\n",
    "        if mask.shape != data.shape:\n",
    "            raise ValueError(\"Mask must have the same shape as data.\")\n",
    "\n",
    "        mask = pd.DataFrame(mask,\n",
    "                            index=data.index,\n",
    "                            columns=data.columns,\n",
    "                            dtype=np.bool)\n",
    "\n",
    "    elif isinstance(mask, pd.DataFrame):\n",
    "        # For DataFrame masks, ensure that semantic labels match data\n",
    "        if not mask.index.equals(data.index) \\\n",
    "           and mask.columns.equals(data.columns):\n",
    "            err = \"Mask must have the same index and columns as data.\"\n",
    "            raise ValueError(err)\n",
    "\n",
    "    # Add any cells with missing data to the mask\n",
    "    # This works around an issue where `plt.pcolormesh` doesn't represent\n",
    "    # missing data properly\n",
    "    mask = mask | pd.isnull(data)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _normalize_cell_size(size, minimum, maximum, alias_true, alias_false, alias_nan):\n",
    "    \"\"\"NEW--\"\"\"\n",
    "    if isinstance(size, bool):\n",
    "        return alias_true if size else alias_false\n",
    "    elif np.isnan(size):\n",
    "        return alias_nan\n",
    "    elif size <= minimum:\n",
    "        return minimum\n",
    "    elif size >= maximum:\n",
    "        return maximum\n",
    "    else:\n",
    "        return size\n",
    "\n",
    "\n",
    "class _CustomCellHeatMapper(object):\n",
    "    \"\"\"NEW--Custom version of _HeatMapper adding the control of the cell size.\"\"\"\n",
    "\n",
    "    DEFAULT_VMIN_CELLS = .1\n",
    "    DEFAULT_VMAX_CELLS = 1\n",
    "\n",
    "    def __init__(self, data, vmin, vmax, cmap, center, robust, annot, fmt,\n",
    "                 annot_kws, cbar, cbar_kws, shape_kws,\n",
    "                 data_cells, vmin_cells, vmax_cells, robust_cells, robust_type='percentile',\n",
    "                 xticklabels=True, yticklabels=True, mask=None, normalize_cells=True):\n",
    "        \"\"\"\n",
    "            Initialize the plotting object.\n",
    "        \"\"\"\n",
    "        # NEW-- PLOT_DATA\n",
    "        # We always want to have a DataFrame with semantic information\n",
    "        # and an ndarray to pass to matplotlib\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            plot_data = data.values\n",
    "        else:\n",
    "            plot_data = np.asarray(data)\n",
    "            data = pd.DataFrame(plot_data)\n",
    "\n",
    "\n",
    "        # We always want to have a DataFrame with semantic information\n",
    "        # and an ndarray to pass to matplotlib\n",
    "        if data_cells is None:\n",
    "            data_cells = pd.DataFrame(data=np.ones(data.shape, dtype=float),\n",
    "                                      columns=data.columns,\n",
    "                                      index=data.index)\n",
    "\n",
    "        if isinstance(data_cells, pd.DataFrame):\n",
    "            plot_cells = data_cells.values\n",
    "        else:\n",
    "            plot_cells = np.asarray(data_cells)\n",
    "            data_cells = pd.DataFrame(plot_cells)\n",
    "\n",
    "        \n",
    "        # NEW-- PLOT_CELLS\n",
    "        # Validate the mask and convert to DataFrame\n",
    "        mask = _matrix_mask(data, mask)\n",
    "\n",
    "        plot_data = np.ma.masked_where(np.asarray(mask), plot_data)\n",
    "        plot_cells = np.ma.masked_where(np.asarray(mask), plot_cells)\n",
    "\n",
    "\n",
    "        # Get good names for the rows and columns\n",
    "        xtickevery = 1\n",
    "        if isinstance(xticklabels, int):\n",
    "            xtickevery = xticklabels\n",
    "            xticklabels = _index_to_ticklabels(data.columns)\n",
    "        elif xticklabels is True:\n",
    "            xticklabels = _index_to_ticklabels(data.columns)\n",
    "        elif xticklabels is False:\n",
    "            xticklabels = []\n",
    "\n",
    "        ytickevery = 1\n",
    "        if isinstance(yticklabels, int):\n",
    "            ytickevery = yticklabels\n",
    "            yticklabels = _index_to_ticklabels(data.index)\n",
    "        elif yticklabels is True:\n",
    "            yticklabels = _index_to_ticklabels(data.index)\n",
    "        elif yticklabels is False:\n",
    "            yticklabels = []\n",
    "\n",
    "        # Get the positions and used label for the ticks\n",
    "        nx, ny = data.T.shape\n",
    "\n",
    "        if not len(xticklabels):\n",
    "            self.xticks = []\n",
    "            self.xticklabels = []\n",
    "        elif isinstance(xticklabels, string_types) and xticklabels == \"auto\":\n",
    "            self.xticks = \"auto\"\n",
    "            self.xticklabels = _index_to_ticklabels(data.columns)\n",
    "        else:\n",
    "            self.xticks, self.xticklabels = self._skip_ticks(xticklabels,\n",
    "                                                             xtickevery)\n",
    "\n",
    "        if not len(yticklabels):\n",
    "            self.yticks = []\n",
    "            self.yticklabels = []\n",
    "        elif isinstance(yticklabels, string_types) and yticklabels == \"auto\":\n",
    "            self.yticks = \"auto\"\n",
    "            self.yticklabels = _index_to_ticklabels(data.index)\n",
    "        else:\n",
    "            self.yticks, self.yticklabels = self._skip_ticks(yticklabels,\n",
    "                                                             ytickevery)\n",
    "\n",
    "        # Get good names for the axis labels\n",
    "        xlabel = _index_to_label(data.columns)\n",
    "        ylabel = _index_to_label(data.index)\n",
    "        self.xlabel = xlabel if xlabel is not None else \"\"\n",
    "        self.ylabel = ylabel if ylabel is not None else \"\"\n",
    "\n",
    "        # Determine good default values for the colormapping\n",
    "        self._determine_cmap_params(plot_data, vmin, vmax,\n",
    "                                    cmap, center, robust)\n",
    "\n",
    "        # Determine good default values for the sizemapping according to a feature\n",
    "        self._determine_cells_params(plot_cells, vmin_cells,\n",
    "                                     vmax_cells, robust_cells,\n",
    "                                     robust_type, normalize_cells)\n",
    "\n",
    "        # Sort out the annotations\n",
    "        if annot is None:\n",
    "            annot = False\n",
    "            annot_data = None\n",
    "        elif isinstance(annot, bool):\n",
    "            if annot:\n",
    "                annot_data = plot_data\n",
    "            else:\n",
    "                annot_data = None\n",
    "        else:\n",
    "            try:\n",
    "                annot_data = annot.values\n",
    "            except AttributeError:\n",
    "                annot_data = annot\n",
    "            if annot.shape != plot_data.shape:\n",
    "                raise ValueError('Data supplied to \"annot\" must be the same '\n",
    "                                 'shape as the data to plot.')\n",
    "            annot = True\n",
    "\n",
    "        # Save other attributes to the object\n",
    "        self.data = data\n",
    "        self.plot_data = plot_data\n",
    "\n",
    "        # NEW-- Save other attributes to the object\n",
    "        self.data_cells = data_cells\n",
    "        self.plot_cells = plot_cells\n",
    "\n",
    "        self.annot = annot\n",
    "        self.annot_data = annot_data\n",
    "\n",
    "        self.fmt = fmt\n",
    "        self.annot_kws = {} if annot_kws is None else annot_kws\n",
    "        self.cbar = cbar\n",
    "        self.cbar_kws = {} if cbar_kws is None else cbar_kws\n",
    "        self.cbar_kws.setdefault('ticks', mpl.ticker.MaxNLocator(6))\n",
    "        # NEW-- \n",
    "        self.shape_kws = {} if shape_kws is None else shape_kws\n",
    "\n",
    "    def _determine_cmap_params(self, plot_data, vmin, vmax,\n",
    "                               cmap, center, robust):\n",
    "        \"\"\"Use some heuristics to set good defaults for colorbar and range.\"\"\"\n",
    "        calc_data = plot_data.data[~np.isnan(plot_data.data)]\n",
    "        if vmin is None:\n",
    "            vmin = np.percentile(calc_data, 2) if robust else calc_data.min()\n",
    "        if vmax is None:\n",
    "            vmax = np.percentile(calc_data, 98) if robust else calc_data.max()\n",
    "        self.vmin, self.vmax = vmin, vmax\n",
    "\n",
    "        # Choose default colormaps if not provided\n",
    "        if cmap is None:\n",
    "            if center is None:\n",
    "                self.cmap = cm.rocket\n",
    "            else:\n",
    "                self.cmap = cm.icefire\n",
    "        elif isinstance(cmap, string_types):\n",
    "            self.cmap = mpl.cm.get_cmap(cmap)\n",
    "        elif isinstance(cmap, list):\n",
    "            self.cmap = mpl.colors.ListedColormap(cmap)\n",
    "        else:\n",
    "            self.cmap = cmap\n",
    "\n",
    "        # Recenter a divergent colormap\n",
    "        if center is not None:\n",
    "            vrange = max(vmax - center, center - vmin)\n",
    "            normlize = mpl.colors.Normalize(center - vrange, center + vrange)\n",
    "            cmin, cmax = normlize([vmin, vmax])\n",
    "            cc = np.linspace(cmin, cmax, 256)\n",
    "            self.cmap = mpl.colors.ListedColormap(self.cmap(cc))\n",
    "\n",
    "    def _determine_cells_params(self, plot_cells, vmin_cells, vmax_cells, robust_cells, robust_type, normalize_cells):\n",
    "        \"\"\" NEW-- Use some heuristics to set good defaults for cells' size.\"\"\"\n",
    "        # Handle unknown robust methods\n",
    "        if robust_type not in [ 'percentile', 'boundary' ] and robust_cells:\n",
    "            raise ValueError(f\"Incorrect robust_type: {robust_type} instead of 'percentile' or 'boundary.\")\n",
    "\n",
    "        # Handle incorrect types (only accepted or np.bool and np.numeric)\n",
    "        type_cells = [ list(map(type,l)) for l in plot_cells ]\n",
    "        available_types = functools.reduce(operator.xor, map(set, type_cells))\n",
    "        invalid_types = [ _type for _type in available_types if not issubclass(_type, (bool, numbers.Number)) ]\n",
    "\n",
    "        if invalid_types:\n",
    "            raise TypeError(f\"Incorrect types: {invalid_types}.\")\n",
    "\n",
    "        calc_cells = plot_cells.data[~(np.isnan(plot_cells.data))]\n",
    "\n",
    "        # Compute vmin_cells and vmax_cells according the method\n",
    "        robust_vmax_cells, robust_vmin_cells = None, None \n",
    "        if robust_cells:\n",
    "            if robust_type == 'percentile':\n",
    "                robust_vmax_cells = np.percentile(calc_cells, 5)\n",
    "                robust_vmin_cells = np.percentile(calc_cells, 95)\n",
    "            \n",
    "            if robust_type == 'boundary':\n",
    "                robust_vmax_cells = calc_cells.min()\n",
    "                robust_vmin_cells = calc_cells.max()\n",
    "        \n",
    "        self.vmax_cells = robust_vmax_cells or self.DEFAULT_VMAX_CELLS\n",
    "        self.vmin_cells = robust_vmin_cells or self.DEFAULT_VMIN_CELLS\n",
    "\n",
    "        # Normalize the values and format into a unique type with the right imputation\n",
    "        normalize = lambda x: _normalize_cell_size(\n",
    "                                         size=x,\n",
    "                                         minimum=self.vmin_cells,\n",
    "                                         maximum=self.vmax_cells,\n",
    "                                         alias_true=self.vmax_cells,\n",
    "                                         alias_false=self.vmin_cells,\n",
    "                                         alias_nan=0.0\n",
    "                                        )                             \n",
    "\n",
    "        plot_cells = np.ma.masked_array(data=[ list(map(normalize, l)) for l in plot_cells.data ],\n",
    "                                        mask=plot_cells.mask)\n",
    "\n",
    "        # Store the values\n",
    "        self.plot_cells = plot_cells\n",
    "\n",
    "    def _annotate_and_size_cells(self, ax, mesh, square_shaped_cells):\n",
    "        \"\"\"Add textual labels with the value in each cell.\"\"\"\n",
    "        # ( MODIFY: former _annotate_heatmap )\n",
    "        mesh.update_scalarmappable()\n",
    "        annot_data = self.annot_data or np.full(self.plot_data.shape, np.nan)\n",
    "        height, width = self.plot_data.shape\n",
    "        xpos, ypos = np.meshgrid(np.arange(width) + .5, np.arange(height) + .5)\n",
    "        for x, y, m, color, annotation, cell_size in zip(xpos.flat, ypos.flat,\n",
    "                                                  mesh.get_array(), mesh.get_facecolors(),\n",
    "                                                  annot_data.flat, self.plot_cells.flat):\n",
    "            if m is not np.ma.masked:\n",
    "                size = np.clip ( cell_size / self.vmax_cells, 0.1, 1.0)\n",
    "                shape = None\n",
    "                if square_shaped_cells:\n",
    "                    shape = plt.Rectangle((x - size / 2, y - size / 2),\n",
    "                                          size,\n",
    "                                          size,\n",
    "                                          facecolor=color,\n",
    "                                          **self.shape_kws)\n",
    "                else:\n",
    "                    shape = plt.Circle((x - size / 2, y - size / 2),\n",
    "                                       size,\n",
    "                                       facecolor=color,\n",
    "                                       fill=True,\n",
    "                                       **self.shape_kws)\n",
    "\n",
    "                ax.add_patch(shape)\n",
    "\n",
    "                if self.annot and not np.isnan(annotation):\n",
    "                    lum = relative_luminance(color)\n",
    "                    text_color = \".15\" if lum > .408 else \"w\"\n",
    "                    annotation = (\"{:\" + self.fmt + \"}\").format(annotation)\n",
    "                    text_kwargs = dict(\n",
    "                        color=text_color, ha=\"center\", va=\"center\")\n",
    "                    text_kwargs.update(self.annot_kws)\n",
    "                    ax.text(x, y, annotation, **text_kwargs)\n",
    "\n",
    "    def _skip_ticks(self, labels, tickevery):\n",
    "        \"\"\"Return ticks and labels at evenly spaced intervals.\"\"\"\n",
    "        n = len(labels)\n",
    "        if tickevery == 0:\n",
    "            ticks, labels = [], []\n",
    "        elif tickevery == 1:\n",
    "            ticks, labels = np.arange(n) + .5, labels\n",
    "        else:\n",
    "            start, end, step = 0, n, tickevery\n",
    "            ticks = np.arange(start, end, step) + .5\n",
    "            labels = labels[start:end:step]\n",
    "        return ticks, labels\n",
    "\n",
    "    def _auto_ticks(self, ax, labels, axis):\n",
    "        \"\"\"Determine ticks and ticklabels that minimize overlap.\"\"\"\n",
    "        transform = ax.figure.dpi_scale_trans.inverted()\n",
    "        bbox = ax.get_window_extent().transformed(transform)\n",
    "        size = [bbox.width, bbox.height][axis]\n",
    "        axis = [ax.xaxis, ax.yaxis][axis]\n",
    "        tick, = axis.set_ticks([0])\n",
    "        fontsize = tick.label1.get_size()\n",
    "        max_ticks = int(size // (fontsize / 72))\n",
    "        if max_ticks < 1:\n",
    "            return [], []\n",
    "        tick_every = len(labels) // max_ticks + 1\n",
    "        tick_every = 1 if tick_every == 0 else tick_every\n",
    "        ticks, labels = self._skip_ticks(labels, tick_every)\n",
    "        return ticks, labels\n",
    "\n",
    "    def plot(self, ax, cax, square_shaped_cells, **kwargs):\n",
    "        \"\"\"Draw the heatmap on the provided Axes.\"\"\"\n",
    "        # Remove all the Axes spines\n",
    "        despine(ax=ax, left=True, bottom=True)\n",
    "\n",
    "        # Draw the heatmap\n",
    "        mesh = ax.pcolormesh(self.plot_data, vmin=self.vmin, vmax=self.vmax,\n",
    "                             cmap=self.cmap, **kwargs)\n",
    "\n",
    "        # Set the axis limits\n",
    "        ax.set(xlim=(0, self.data.shape[1]), ylim=(0, self.data.shape[0]))\n",
    "\n",
    "        # Invert the y axis to show the plot in matrix form\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        # Possibly add a colorbar\n",
    "        if self.cbar:\n",
    "            cb = ax.figure.colorbar(mesh, cax, ax, **self.cbar_kws)\n",
    "            cb.outline.set_linewidth(0)\n",
    "            # If rasterized is passed to pcolormesh, also rasterize the\n",
    "            # colorbar to avoid white lines on the PDF rendering\n",
    "            if kwargs.get('rasterized', False):\n",
    "                cb.solids.set_rasterized(True)\n",
    "\n",
    "        # Add row and column labels\n",
    "        if isinstance(self.xticks, string_types) and self.xticks == \"auto\":\n",
    "            xticks, xticklabels = self._auto_ticks(ax, self.xticklabels, 0)\n",
    "        else:\n",
    "            xticks, xticklabels = self.xticks, self.xticklabels\n",
    "\n",
    "        if isinstance(self.yticks, string_types) and self.yticks == \"auto\":\n",
    "            yticks, yticklabels = self._auto_ticks(ax, self.yticklabels, 1)\n",
    "        else:\n",
    "            yticks, yticklabels = self.yticks, self.yticklabels\n",
    "\n",
    "        ax.set(xticks=xticks, yticks=yticks)\n",
    "        xtl = ax.set_xticklabels(xticklabels)\n",
    "        ytl = ax.set_yticklabels(yticklabels, rotation=\"vertical\")\n",
    "\n",
    "        # Possibly rotate them if they overlap\n",
    "        if hasattr(ax.figure.canvas, \"get_renderer\"):\n",
    "            ax.figure.draw(ax.figure.canvas.get_renderer())\n",
    "        if axis_ticklabels_overlap(xtl):\n",
    "            plt.setp(xtl, rotation=\"vertical\")\n",
    "        if axis_ticklabels_overlap(ytl):\n",
    "            plt.setp(ytl, rotation=\"horizontal\")\n",
    "\n",
    "        # Add the axis labels\n",
    "        ax.set(xlabel=self.xlabel, ylabel=self.ylabel)\n",
    "\n",
    "        # Annotate the cells with the formatted values\n",
    "        self._annotate_and_size_cells(ax, mesh, square_shaped_cells)\n",
    "\n",
    "\n",
    "def custom_cells_heatmap(data, vmin=None, vmax=None, cmap=None, center=None, robust=False,\n",
    "                        annot=None, fmt=\".2g\", annot_kws=None,\n",
    "                        linewidths=0, linecolor=\"white\",\n",
    "                        cbar=True, cbar_kws=None, cbar_ax=None,\n",
    "                        square=False, xticklabels=\"auto\", yticklabels=\"auto\",\n",
    "                        mask=None, ax=None, data_cells=None, robust_cells=True,\n",
    "                        robust_type='percentile', vmin_cells=None, vmax_cells=None,\n",
    "                        shape_kws=None, normalize_cells=True,\n",
    "                        square_shaped_cells=True, **kwargs):\n",
    "    \"\"\" NEW-- Tweak the function heatmap by adding a third variable.\"\"\"\n",
    "    # Initialize the plotter object\n",
    "    plotter =  _CustomCellHeatMapper(data, vmin, vmax, cmap, center,\n",
    "                robust, annot, fmt, annot_kws, cbar, cbar_kws, shape_kws,\n",
    "                data_cells, vmin_cells, vmax_cells, robust_cells,\n",
    "                robust_type, xticklabels, yticklabels, mask,\n",
    "                normalize_cells)\n",
    "\n",
    "    # Add the pcolormesh kwargs here\n",
    "    kwargs[\"linewidths\"] = linewidths\n",
    "    kwargs[\"edgecolor\"] = linecolor\n",
    "\n",
    "    # Draw the plot and return the Axes\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    if square:\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "    # delete grid\n",
    "    # ax.grid(False)   \n",
    "\n",
    "    plotter.plot(ax, cbar_ax, square_shaped_cells, **kwargs)\n",
    "    return ax\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edaviz",
   "language": "python",
   "name": "edaviz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
